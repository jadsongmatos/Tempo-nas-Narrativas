{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03da0c9a3ab04a8ca4c3e42932df6d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content1</th>\n",
       "      <th>content2</th>\n",
       "      <th>target</th>\n",
       "      <th>target_transformed</th>\n",
       "      <th>target_transformed_sigmoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.expressdecor.com/review/product/lis...</td>\n",
       "      <td>for our detailed return policy here. You have ...</td>\n",
       "      <td>please view the following link</td>\n",
       "      <td>31</td>\n",
       "      <td>3.465736</td>\n",
       "      <td>0.969697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.rug.nl/research/portal/publication...</td>\n",
       "      <td>risk of being killed by</td>\n",
       "      <td>isolation. 4. The frequency of the rhythm may ...</td>\n",
       "      <td>516</td>\n",
       "      <td>6.248043</td>\n",
       "      <td>0.998069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://theswimit.com/products/premium-swim-it...</td>\n",
       "      <td>is armed with a good</td>\n",
       "      <td>include an integrated \"rubber weave\" to keep t...</td>\n",
       "      <td>174</td>\n",
       "      <td>5.164786</td>\n",
       "      <td>0.994318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.osnews.com/permalink?569456</td>\n",
       "      <td>read all comments associated with this story, ...</td>\n",
       "      <td>familiar with Microsoft's Windows Phone plans ...</td>\n",
       "      <td>390</td>\n",
       "      <td>5.968708</td>\n",
       "      <td>0.997449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.gordonsjewelers.com/product/index.j...</td>\n",
       "      <td>prices, styles and availability may</td>\n",
       "      <td>to show detail and may</td>\n",
       "      <td>70</td>\n",
       "      <td>4.262680</td>\n",
       "      <td>0.986111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78322</th>\n",
       "      <td>http://www.naplesnews.com/news/2013/jan/24/new...</td>\n",
       "      <td>builds to ensure construction activities</td>\n",
       "      <td>Virginia Rochelle has joined the management te...</td>\n",
       "      <td>367</td>\n",
       "      <td>5.908083</td>\n",
       "      <td>0.997290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78323</th>\n",
       "      <td>http://www.aspca.org/blog/terrified-matted-pup...</td>\n",
       "      <td>make a wonderful companion. Soon</td>\n",
       "      <td>rescues or private shelters by providing munic...</td>\n",
       "      <td>504</td>\n",
       "      <td>6.224558</td>\n",
       "      <td>0.998024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78324</th>\n",
       "      <td>http://www.uvm.edu/~uvmpr/?Page=news&amp;storyID=1...</td>\n",
       "      <td>applications for water resources and irrigatio...</td>\n",
       "      <td>work at UVM on Aug.</td>\n",
       "      <td>2359</td>\n",
       "      <td>7.766417</td>\n",
       "      <td>0.999576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78325</th>\n",
       "      <td>https://magazine.wsu.edu/web-extra/butchmen-me...</td>\n",
       "      <td>that, when we went to load him up we didn't</td>\n",
       "      <td>a student in a costume.</td>\n",
       "      <td>-5543</td>\n",
       "      <td>-8.620472</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78326</th>\n",
       "      <td>Aristoteles em nova perspectiva - Olavo de Car...</td>\n",
       "      <td>a polarização das premissas (e portanto dos de...</td>\n",
       "      <td>— ou para a frente, na série indefinida das</td>\n",
       "      <td>-1328</td>\n",
       "      <td>-7.192182</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78327 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "0      http://www.expressdecor.com/review/product/lis...   \n",
       "1      https://www.rug.nl/research/portal/publication...   \n",
       "2      https://theswimit.com/products/premium-swim-it...   \n",
       "3                 http://www.osnews.com/permalink?569456   \n",
       "4      http://www.gordonsjewelers.com/product/index.j...   \n",
       "...                                                  ...   \n",
       "78322  http://www.naplesnews.com/news/2013/jan/24/new...   \n",
       "78323  http://www.aspca.org/blog/terrified-matted-pup...   \n",
       "78324  http://www.uvm.edu/~uvmpr/?Page=news&storyID=1...   \n",
       "78325  https://magazine.wsu.edu/web-extra/butchmen-me...   \n",
       "78326  Aristoteles em nova perspectiva - Olavo de Car...   \n",
       "\n",
       "                                                content1  \\\n",
       "0      for our detailed return policy here. You have ...   \n",
       "1                                risk of being killed by   \n",
       "2                                   is armed with a good   \n",
       "3      read all comments associated with this story, ...   \n",
       "4                    prices, styles and availability may   \n",
       "...                                                  ...   \n",
       "78322           builds to ensure construction activities   \n",
       "78323                   make a wonderful companion. Soon   \n",
       "78324  applications for water resources and irrigatio...   \n",
       "78325        that, when we went to load him up we didn't   \n",
       "78326  a polarização das premissas (e portanto dos de...   \n",
       "\n",
       "                                                content2  target  \\\n",
       "0                         please view the following link      31   \n",
       "1      isolation. 4. The frequency of the rhythm may ...     516   \n",
       "2      include an integrated \"rubber weave\" to keep t...     174   \n",
       "3      familiar with Microsoft's Windows Phone plans ...     390   \n",
       "4                                 to show detail and may      70   \n",
       "...                                                  ...     ...   \n",
       "78322  Virginia Rochelle has joined the management te...     367   \n",
       "78323  rescues or private shelters by providing munic...     504   \n",
       "78324                                work at UVM on Aug.    2359   \n",
       "78325                            a student in a costume.   -5543   \n",
       "78326        — ou para a frente, na série indefinida das   -1328   \n",
       "\n",
       "       target_transformed  target_transformed_sigmoid  \n",
       "0                3.465736                    0.969697  \n",
       "1                6.248043                    0.998069  \n",
       "2                5.164786                    0.994318  \n",
       "3                5.968708                    0.997449  \n",
       "4                4.262680                    0.986111  \n",
       "...                   ...                         ...  \n",
       "78322            5.908083                    0.997290  \n",
       "78323            6.224558                    0.998024  \n",
       "78324            7.766417                    0.999576  \n",
       "78325           -8.620472                    0.000180  \n",
       "78326           -7.192182                    0.000752  \n",
       "\n",
       "[78327 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Caminhos para os bancos de dados\n",
    "db1_path = 'fineweb.duckdb'  # Substitua pelo caminho do seu primeiro banco de dados\n",
    "db2_path = 'books.duckdb'  # Substitua pelo caminho do seu segundo banco de dados\n",
    "\n",
    "# Alias para o segundo banco de dados\n",
    "alias_db2 = 'db2_alias'\n",
    "\n",
    "# Conectar ao banco de dados principal\n",
    "conn = duckdb.connect(database=db1_path, read_only=False)\n",
    "\n",
    "# Anexar o segundo banco de dados\n",
    "conn.execute(f\"ATTACH DATABASE '{db2_path}' AS {alias_db2}\")\n",
    "\n",
    "# Verificar esquemas (opcional, mas recomendado)\n",
    "schema_main = conn.execute(\"DESCRIBE dataset\").fetchdf()\n",
    "schema_db2 = conn.execute(f\"DESCRIBE {alias_db2}.dataset\").fetchdf()\n",
    "\n",
    "if not schema_main.equals(schema_db2):\n",
    "\traise Exception(\"Os esquemas das tabelas 'dataset' nos dois bancos de dados não são compatíveis.\")\n",
    "\n",
    "# Executar a consulta modificada\n",
    "df_training = conn.execute(f\"\"\"\n",
    "WITH combined_dataset AS (\n",
    "    SELECT * FROM dataset\n",
    "    UNION ALL\n",
    "    SELECT * FROM {alias_db2}.dataset\n",
    "),\n",
    "sampled_a AS (\n",
    "    SELECT id, indice, content, name\n",
    "    FROM (\n",
    "        SELECT\n",
    "            id,\n",
    "            indice,\n",
    "            content,\n",
    "            name,\n",
    "            ROW_NUMBER() OVER (PARTITION BY name ORDER BY RANDOM()) AS rn\n",
    "        FROM combined_dataset\n",
    "    ) sub\n",
    "    WHERE rn <= 3\n",
    "),\n",
    "sampled_b AS (\n",
    "    SELECT id, indice, content, name\n",
    "    FROM (\n",
    "        SELECT\n",
    "            id,\n",
    "            indice,\n",
    "            content,\n",
    "            name,\n",
    "            ROW_NUMBER() OVER (PARTITION BY name ORDER BY RANDOM()) AS rn\n",
    "        FROM combined_dataset\n",
    "    ) sub\n",
    "    WHERE rn <= 3\n",
    ")\n",
    "SELECT\n",
    "    a.name AS name,\n",
    "    a.content AS content1,\n",
    "    b.content AS content2,\n",
    "\ta.indice - b.indice as target,\n",
    "    -- Cálculo original do target_transformado\n",
    "    SIGN(a.indice - b.indice) * LN(1 + ABS(a.indice - b.indice)) AS target_transformed,\n",
    "    -- Aplicação da função sigmoide no target_transformado\n",
    "    1 / (1 + EXP(- (SIGN(a.indice - b.indice) * LN(1 + ABS(a.indice - b.indice))))) AS target_transformed_sigmoid\n",
    "FROM sampled_a a\n",
    "JOIN sampled_b b\n",
    "    ON a.name = b.name\n",
    "ORDER BY RANDOM()\n",
    "--LIMIT 2000;\n",
    "\"\"\").df()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df_training['target_transformed'], bins=50, alpha=0.5, label='target_transformed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content1</th>\n",
       "      <th>content2</th>\n",
       "      <th>target</th>\n",
       "      <th>target_transformed</th>\n",
       "      <th>target_transformed_sigmoid</th>\n",
       "      <th>target_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.expressdecor.com/review/product/lis...</td>\n",
       "      <td>for our detailed return policy here. You have ...</td>\n",
       "      <td>please view the following link</td>\n",
       "      <td>31</td>\n",
       "      <td>3.465736</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.562331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.rug.nl/research/portal/publication...</td>\n",
       "      <td>risk of being killed by</td>\n",
       "      <td>isolation. 4. The frequency of the rhythm may ...</td>\n",
       "      <td>516</td>\n",
       "      <td>6.248043</td>\n",
       "      <td>0.998069</td>\n",
       "      <td>1.009327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://theswimit.com/products/premium-swim-it...</td>\n",
       "      <td>is armed with a good</td>\n",
       "      <td>include an integrated \"rubber weave\" to keep t...</td>\n",
       "      <td>174</td>\n",
       "      <td>5.164786</td>\n",
       "      <td>0.994318</td>\n",
       "      <td>0.835294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.osnews.com/permalink?569456</td>\n",
       "      <td>read all comments associated with this story, ...</td>\n",
       "      <td>familiar with Microsoft's Windows Phone plans ...</td>\n",
       "      <td>390</td>\n",
       "      <td>5.968708</td>\n",
       "      <td>0.997449</td>\n",
       "      <td>0.964450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.gordonsjewelers.com/product/index.j...</td>\n",
       "      <td>prices, styles and availability may</td>\n",
       "      <td>to show detail and may</td>\n",
       "      <td>70</td>\n",
       "      <td>4.262680</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.690365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78322</th>\n",
       "      <td>http://www.naplesnews.com/news/2013/jan/24/new...</td>\n",
       "      <td>builds to ensure construction activities</td>\n",
       "      <td>Virginia Rochelle has joined the management te...</td>\n",
       "      <td>367</td>\n",
       "      <td>5.908083</td>\n",
       "      <td>0.997290</td>\n",
       "      <td>0.954710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78323</th>\n",
       "      <td>http://www.aspca.org/blog/terrified-matted-pup...</td>\n",
       "      <td>make a wonderful companion. Soon</td>\n",
       "      <td>rescues or private shelters by providing munic...</td>\n",
       "      <td>504</td>\n",
       "      <td>6.224558</td>\n",
       "      <td>0.998024</td>\n",
       "      <td>1.005554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78324</th>\n",
       "      <td>http://www.uvm.edu/~uvmpr/?Page=news&amp;storyID=1...</td>\n",
       "      <td>applications for water resources and irrigatio...</td>\n",
       "      <td>work at UVM on Aug.</td>\n",
       "      <td>2359</td>\n",
       "      <td>7.766417</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>1.253264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78325</th>\n",
       "      <td>https://magazine.wsu.edu/web-extra/butchmen-me...</td>\n",
       "      <td>that, when we went to load him up we didn't</td>\n",
       "      <td>a student in a costume.</td>\n",
       "      <td>-5543</td>\n",
       "      <td>-8.620472</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>-1.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78326</th>\n",
       "      <td>Aristoteles em nova perspectiva - Olavo de Car...</td>\n",
       "      <td>a polarização das premissas (e portanto dos de...</td>\n",
       "      <td>— ou para a frente, na série indefinida das</td>\n",
       "      <td>-1328</td>\n",
       "      <td>-7.192182</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>-1.149936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78327 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "0      http://www.expressdecor.com/review/product/lis...   \n",
       "1      https://www.rug.nl/research/portal/publication...   \n",
       "2      https://theswimit.com/products/premium-swim-it...   \n",
       "3                 http://www.osnews.com/permalink?569456   \n",
       "4      http://www.gordonsjewelers.com/product/index.j...   \n",
       "...                                                  ...   \n",
       "78322  http://www.naplesnews.com/news/2013/jan/24/new...   \n",
       "78323  http://www.aspca.org/blog/terrified-matted-pup...   \n",
       "78324  http://www.uvm.edu/~uvmpr/?Page=news&storyID=1...   \n",
       "78325  https://magazine.wsu.edu/web-extra/butchmen-me...   \n",
       "78326  Aristoteles em nova perspectiva - Olavo de Car...   \n",
       "\n",
       "                                                content1  \\\n",
       "0      for our detailed return policy here. You have ...   \n",
       "1                                risk of being killed by   \n",
       "2                                   is armed with a good   \n",
       "3      read all comments associated with this story, ...   \n",
       "4                    prices, styles and availability may   \n",
       "...                                                  ...   \n",
       "78322           builds to ensure construction activities   \n",
       "78323                   make a wonderful companion. Soon   \n",
       "78324  applications for water resources and irrigatio...   \n",
       "78325        that, when we went to load him up we didn't   \n",
       "78326  a polarização das premissas (e portanto dos de...   \n",
       "\n",
       "                                                content2  target  \\\n",
       "0                         please view the following link      31   \n",
       "1      isolation. 4. The frequency of the rhythm may ...     516   \n",
       "2      include an integrated \"rubber weave\" to keep t...     174   \n",
       "3      familiar with Microsoft's Windows Phone plans ...     390   \n",
       "4                                 to show detail and may      70   \n",
       "...                                                  ...     ...   \n",
       "78322  Virginia Rochelle has joined the management te...     367   \n",
       "78323  rescues or private shelters by providing munic...     504   \n",
       "78324                                work at UVM on Aug.    2359   \n",
       "78325                            a student in a costume.   -5543   \n",
       "78326        — ou para a frente, na série indefinida das   -1328   \n",
       "\n",
       "       target_transformed  target_transformed_sigmoid  target_norm  \n",
       "0                3.465736                    0.969697     0.562331  \n",
       "1                6.248043                    0.998069     1.009327  \n",
       "2                5.164786                    0.994318     0.835294  \n",
       "3                5.968708                    0.997449     0.964450  \n",
       "4                4.262680                    0.986111     0.690365  \n",
       "...                   ...                         ...          ...  \n",
       "78322            5.908083                    0.997290     0.954710  \n",
       "78323            6.224558                    0.998024     1.005554  \n",
       "78324            7.766417                    0.999576     1.253264  \n",
       "78325           -8.620472                    0.000180    -1.379400  \n",
       "78326           -7.192182                    0.000752    -1.149936  \n",
       "\n",
       "[78327 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Escalonar os dados\n",
    "scaler = StandardScaler()\n",
    "df_training['target_norm'] = scaler.fit_transform(df_training[['target_transformed']])\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_training['target_norm'], bins=50, alpha=0.5, label='target_transformed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_threshold: 0.16619341941236312\n",
      "positive_threshold: -0.1551201098911789\n",
      "Valor revertido: [[1.0], [-1.0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "single_value_df = pd.DataFrame([[1.0],[-1.0]], columns=['target_transformed'])\n",
    "\n",
    "valor_escalonado = scaler.transform(single_value_df)\n",
    "\n",
    "# Definir os limites para considerar positivo ou negativo\n",
    "negative_threshold = valor_escalonado[0][0]\n",
    "positive_threshold = valor_escalonado[1][0]\n",
    "\n",
    "print(\"negative_threshold:\", negative_threshold)\n",
    "print(\"positive_threshold:\", positive_threshold)\n",
    "\n",
    "# Reverter a transformação\n",
    "valor_revertido = scaler.inverse_transform(valor_escalonado)\n",
    "print(\"Valor revertido:\", valor_revertido.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"CUDA is not available. Please ensure you have a compatible GPU and drivers installed.\")\n",
    "else:\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Definir paralelismo corretamente\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "class TextToEmbedding:\n",
    "    def __init__(self, weights_path, num_ids=128256, vector_size=2048, device='cpu'):\n",
    "        \"\"\"\n",
    "        Inicializa a classe TextToEmbedding sem padding fixo.\n",
    "\n",
    "        Args:\n",
    "            weights_path (str): Caminho para o arquivo .npy que contém os pesos.\n",
    "            num_ids (int, opcional): Número total de IDs. Padrão é 128256.\n",
    "            vector_size (int, opcional): Tamanho de cada vetor de embedding. Padrão é 2048.\n",
    "            device (str, opcional): Dispositivo para carregar os tensores ('cpu' ou 'cuda'). Padrão é 'cpu'.\n",
    "        \"\"\"\n",
    "\t\t\n",
    "        self.device = device\n",
    "\n",
    "        # Carrega o tokenizer sem padding fixo\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B\", use_fast=True)\n",
    "        \n",
    "        # Carrega os pesos a partir do arquivo .npy\n",
    "        try:\n",
    "            weights_np = np.load(weights_path)\n",
    "            self.weights = torch.from_numpy(weights_np).to(self.device)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"O arquivo de pesos '{weights_path}' não foi encontrado.\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Erro ao carregar os pesos: {e}\")\n",
    "        \n",
    "        # Verifica a forma dos pesos\n",
    "        if self.weights.shape != (num_ids, vector_size):\n",
    "            raise ValueError(f\"O formato do arquivo weights.npy é {self.weights.shape}, mas era esperado {(num_ids, vector_size)}.\")\n",
    "\n",
    "embedding_generator = TextToEmbedding(\"weights_half.npy\", device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SequenceDataset_val(Dataset):\n",
    "\tdef __init__(self, df):\n",
    "\t\tself.df = df.reset_index(drop=True)\n",
    "\n",
    "\t\tdf['target_norm'] = scaler.fit_transform(df[['target_transformed']])\n",
    "\n",
    "\t\t# Defina o número de elementos originais em cada coluna\n",
    "\t\tnum_content1 = len(df['content1'])\n",
    "\n",
    "\t\t# Combinar as duas colunas de conteúdo em uma lista\n",
    "\t\tcontents = list(df['content1']) + list(df['content2'])\n",
    "\n",
    "\t\t# Tokenização combinada\n",
    "\t\tencoding = embedding_generator.tokenizer(\n",
    "\t\t\tcontents,\n",
    "\t\t\treturn_tensors=\"pt\",\n",
    "\t\t\tpadding=True,  # Padding dinâmico\n",
    "\t\t\ttruncation=True\n",
    "\t\t)\n",
    "\n",
    "\t\t# Separar os encodings de volta para cada conjunto\n",
    "\t\tencoding1 = {key: encoding[key][:num_content1] for key in encoding.keys()}\n",
    "\t\tencoding2 = {key: encoding[key][num_content1:] for key in encoding.keys()}\n",
    "\t\t\n",
    "\t\t# Converter para o dispositivo\n",
    "\t\tinput_ids1 = encoding1['input_ids'].to(embedding_generator.device)\n",
    "\t\tattention_mask1 = encoding1['attention_mask'].to(embedding_generator.device)\n",
    "\t\tinput_ids2 = encoding2['input_ids'].to(embedding_generator.device)\n",
    "\t\tattention_mask2 = encoding2['attention_mask'].to(embedding_generator.device)\n",
    "\t\t\n",
    "\t\t# Obter embeddings\n",
    "\t\tembeddings1 = embedding_generator.weights[input_ids1]\n",
    "\t\tembeddings2 = embedding_generator.weights[input_ids2]\n",
    "\t\t\n",
    "\t\t# Concatenar as duas sequências no eixo 1\n",
    "\t\tself.embeddings = torch.stack([embeddings1, embeddings2], dim=1)  # (batch_size, 2, seq_length, vector_size)\n",
    "\t\t\n",
    "\t\t# Calcular comprimentos\n",
    "\t\tlengths1 = attention_mask1.sum(dim=1)\n",
    "\t\tlengths2 = attention_mask2.sum(dim=1)\n",
    "\t\tself.lengths = torch.stack([lengths1, lengths2], dim=1)  # (batch_size, 2)\n",
    "\n",
    "\t\t# Converter targets\n",
    "\t\tself.targets = torch.tensor(df['target_norm'], dtype=torch.float16)  # (batch_size,)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.df)\n",
    "\t\n",
    "\tdef __getitem__(self,idx):\n",
    "\t\treturn self.embeddings[idx], self.lengths[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to the databases\n",
    "db1_path = 'fineweb.duckdb'\n",
    "db2_path = 'books.duckdb'\n",
    "batch_size = 150\n",
    "\n",
    "class IterableSequenceDataset(IterableDataset):\n",
    "\tdef __init__(self, db1_path, db2_path, batch_size=150):\n",
    "\t\tsuper(IterableSequenceDataset, self).__init__()\n",
    "\t\tself.db1_path = db1_path\n",
    "\t\tself.db2_path = db2_path\n",
    "\t\tself.batch_size = batch_size\n",
    "\n",
    "\tdef _fetch_data(self, conn):\n",
    "\t\tquery = f\"\"\"\n",
    "\t\t\tWITH combined_dataset AS (\n",
    "\t\t\t\tSELECT * FROM main.dataset\n",
    "\t\t\t\tUNION ALL\n",
    "\t\t\t\tSELECT * FROM db2.dataset\n",
    "\t\t\t),\n",
    "\t\t\tsampled_a AS (\n",
    "\t\t\t\tSELECT id, indice, content, name\n",
    "\t\t\t\tFROM (\n",
    "\t\t\t\t\tSELECT\n",
    "\t\t\t\t\t\tid,\n",
    "\t\t\t\t\t\tindice,\n",
    "\t\t\t\t\t\tcontent,\n",
    "\t\t\t\t\t\tname,\n",
    "\t\t\t\t\t\tROW_NUMBER() OVER (PARTITION BY name ORDER BY RANDOM()) AS rn\n",
    "\t\t\t\t\tFROM combined_dataset\n",
    "\t\t\t\t) sub\n",
    "\t\t\t\tWHERE rn <= 1\n",
    "\t\t\t),\n",
    "\t\t\tsampled_b AS (\n",
    "\t\t\t\tSELECT id, indice, content, name\n",
    "\t\t\t\tFROM (\n",
    "\t\t\t\t\tSELECT\n",
    "\t\t\t\t\t\tid,\n",
    "\t\t\t\t\t\tindice,\n",
    "\t\t\t\t\t\tcontent,\n",
    "\t\t\t\t\t\tname,\n",
    "\t\t\t\t\t\tROW_NUMBER() OVER (PARTITION BY name ORDER BY RANDOM()) AS rn\n",
    "\t\t\t\t\tFROM combined_dataset\n",
    "\t\t\t\t) sub\n",
    "\t\t\t\tWHERE rn <= 1\n",
    "\t\t\t)\n",
    "\t\t\tSELECT\n",
    "\t\t\t\ta.name AS name,\n",
    "\t\t\t\ta.content AS content1,\n",
    "\t\t\t\tb.content AS content2,\n",
    "\t\t\t\tSIGN(a.indice - b.indice) * LN(1 + ABS(a.indice - b.indice)) AS target_transformed\n",
    "\t\t\tFROM sampled_a a\n",
    "\t\t\tJOIN sampled_b b\n",
    "\t\t\t\tON a.name = b.name\n",
    "\t\t\tORDER BY RANDOM()\n",
    "\t\t\tLIMIT {self.batch_size};\n",
    "\t\t\"\"\"\n",
    "\t\tdf = conn.execute(query).df()\n",
    "\n",
    "\t\tdf['target_norm'] = scaler.transform(df[['target_transformed']])\n",
    "\n",
    "\t\t# Defina o número de elementos originais em cada coluna\n",
    "\t\tnum_content1 = len(df['content1'])\n",
    "\n",
    "\t\t# Combinar as duas colunas de conteúdo em uma lista\n",
    "\t\tcontents = list(df['content1']) + list(df['content2'])\n",
    "\n",
    "\t\t# Tokenização combinada\n",
    "\t\tencoding = embedding_generator.tokenizer(\n",
    "\t\t\tcontents,\n",
    "\t\t\treturn_tensors=\"pt\",\n",
    "\t\t\tpadding=True,  # Padding dinâmico\n",
    "\t\t\ttruncation=True\n",
    "\t\t)\n",
    "\n",
    "\t\t# Separar os encodings de volta para cada conjunto\n",
    "\t\tencoding1 = {key: encoding[key][:num_content1] for key in encoding.keys()}\n",
    "\t\tencoding2 = {key: encoding[key][num_content1:] for key in encoding.keys()}\n",
    "\n",
    "\t\t# Converter para o dispositivo\n",
    "\t\tinput_ids1 = encoding1['input_ids'].to(embedding_generator.device)\n",
    "\t\tattention_mask1 = encoding1['attention_mask'].to(embedding_generator.device)\n",
    "\t\tinput_ids2 = encoding2['input_ids'].to(embedding_generator.device)\n",
    "\t\tattention_mask2 = encoding2['attention_mask'].to(embedding_generator.device)\n",
    "\n",
    "\t\t# Obter embeddings\n",
    "\t\tembeddings1 = embedding_generator.weights[input_ids1]\n",
    "\t\tembeddings2 = embedding_generator.weights[input_ids2]\n",
    "\n",
    "\t\t# Concatenar as duas sequências no eixo 1\n",
    "\t\tembeddings = torch.stack([embeddings1, embeddings2], dim=1)  # (batch_size, 2, seq_length, vector_size)\n",
    "\n",
    "\t\t# Calcular comprimentos\n",
    "\t\tlengths1 = attention_mask1.sum(dim=1)\n",
    "\t\tlengths2 = attention_mask2.sum(dim=1)\n",
    "\t\tlengths = torch.stack([lengths1, lengths2], dim=1)  # (batch_size, 2)\n",
    "\n",
    "\t\t# Converter targets\n",
    "\t\ttargets = torch.tensor(df['target_norm'], dtype=torch.float16)  # (batch_size,)\n",
    "\n",
    "\t\treturn embeddings,lengths,targets\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tIterator que gera dados continuamente.\n",
    "\n",
    "\t\tYields:\n",
    "\t\t\ttuple: (content1, content2, target) para cada amostra.\n",
    "\t\t\"\"\"\n",
    "\t\t# Estabelecer conexão com DuckDB\n",
    "\t\tconn = duckdb.connect(database=self.db1_path, read_only=True)\n",
    "\t\tconn.execute(\"SET enable_progress_bar=false\")\n",
    "\t\t\n",
    "\t\t# Anexar o segundo banco de dados com um alias 'db2'\n",
    "\t\tconn.execute(f\"ATTACH '{self.db2_path}' AS db2\")\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\twhile True:\n",
    "\t\t\t\tembeddings,lengths,targets = self._fetch_data(conn)\n",
    "\t\t\t\tfor i in range(batch_size): \n",
    "\t\t\t\t\tyield embeddings[i], lengths[i], targets[i]\n",
    "\t\tfinally:\n",
    "\t\t\t# Garantir que a conexão seja fechada quando o iterador for finalizado\n",
    "\t\t\tconn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "from cleantext import clean\n",
    "\n",
    "# Configurar o conversor\n",
    "converter = html2text.HTML2Text()\n",
    "converter.ignore_links = True\n",
    "converter.ignore_images = True\n",
    "converter.ignore_tables = True\n",
    "\n",
    "def cleartext(decoded_html):\n",
    "\ttexto_limpo = converter.handle(decoded_html)\n",
    "\ttexto_limpo = clean(\n",
    "\t\ttexto_limpo,\n",
    "\t\tfix_unicode=True,\n",
    "\t\tto_ascii=False,\n",
    "\t\tlower=False,\n",
    "\t\tno_line_breaks=True,\n",
    "\t\tno_urls=True,\n",
    "\t\tno_emails=True,\n",
    "\t\tno_phone_numbers=True,\n",
    "\t\tno_numbers=False,\n",
    "\t\tno_digits=False,\n",
    "\t\tno_currency_symbols=True,\n",
    "\t\tno_punct=False,  # Defina como True se quiser remover pontuação\n",
    "\t\treplace_with_punct=\"\",\n",
    "\t\treplace_with_url=\"\",\n",
    "\t\treplace_with_email=\"\",\n",
    "\t\treplace_with_phone_number=\"\",\n",
    "\t\treplace_with_number=\"\",\n",
    "\t\treplace_with_digit=\"\",\n",
    "\t\treplace_with_currency_symbol=\"\"\n",
    "\t)\n",
    "\n",
    "\treturn texto_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Definição da classe TreeNode conforme acima\n",
    "class TreeNode:\n",
    "    def __init__(self, substring: str, position: int, name: str, size: int):\n",
    "        self.substring = substring\n",
    "        self.position = position\n",
    "        self.name = name\n",
    "        self.size = size\n",
    "        self.left: Optional['TreeNode'] = None\n",
    "        self.right: Optional['TreeNode'] = None\n",
    "\n",
    "    def is_leaf(self) -> bool:\n",
    "        return self.size <= 64\n",
    "\n",
    "    def add_children(self, left_child: 'TreeNode', right_child: 'TreeNode'):\n",
    "        self.left = left_child\n",
    "        self.right = right_child\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Converte o nó e seus filhos em um dicionário.\n",
    "        \"\"\"\n",
    "        node_dict = {\n",
    "            'substring': self.substring,\n",
    "            'position': self.position,\n",
    "            'name': self.name,\n",
    "            'size': self.size,\n",
    "            'left': self.left.to_dict() if self.left else None,\n",
    "            'right': self.right.to_dict() if self.right else None\n",
    "        }\n",
    "        return node_dict\n",
    "\n",
    "# Função para salvar a árvore em JSON\n",
    "def save_tree_to_json(root_nodes: List[TreeNode], filename: str):\n",
    "    \"\"\"\n",
    "    Salva a árvore de nós em um arquivo JSON.\n",
    "\n",
    "    Args:\n",
    "        root_nodes (List[TreeNode]): Lista de nós raiz da árvore.\n",
    "        filename (str): Nome do arquivo JSON para salvar a árvore.\n",
    "    \"\"\"\n",
    "    # Converter todos os nós raiz em dicionários\n",
    "    trees_as_dicts = [root.to_dict() for root in root_nodes]\n",
    "\n",
    "    # Salvar a lista de árvores no arquivo JSON\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(trees_as_dicts, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Árvore salva em {filename}\")\n",
    "\n",
    "# Função de divisão recursiva conforme acima\n",
    "def splitter_length_tree_recursive(text: str, name: str, max_size: int = 1024, min_size: int = 64, use_tqdm: bool = True) -> List[TreeNode]:\n",
    "    \"\"\"\n",
    "    Divide o texto em substrings recursivamente pela metade até que cada substring tenha tamanho <= min_size.\n",
    "    \n",
    "    Args:\n",
    "        text (str): O texto a ser dividido.\n",
    "        name (str): Nome associado às substrings.\n",
    "        max_size (int): Tamanho máximo inicial das substrings.\n",
    "        min_size (int): Tamanho mínimo das substrings.\n",
    "        use_tqdm (bool): Se True, exibe uma barra de progresso.\n",
    "    \n",
    "    Returns:\n",
    "        List[TreeNode]: Lista de nós de árvore que representam as substrings.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_size,\n",
    "        chunk_overlap=int(max_size / 5),\n",
    "        length_function=length_function,\n",
    "    )\n",
    "\n",
    "    # Dividir o texto inicial em chunks de tamanho máximo\n",
    "    initial_splits = splitter.split_text(text)\n",
    "    initial_splits = [split for split in initial_splits if len(split) >= 1]  # Garantir comprimento mínimo\n",
    "\n",
    "    if use_tqdm:\n",
    "        iterator = tqdm(initial_splits, desc=f\"Dividindo chunks de tamanho {max_size}\")\n",
    "    else:\n",
    "        iterator = initial_splits\n",
    "\n",
    "    # Criar nós raiz para cada chunk inicial\n",
    "    root_nodes = []\n",
    "    for split in iterator:\n",
    "        indice = text.find(split)\n",
    "        if indice == -1:\n",
    "            print(f\"Split não encontrado: {split[:30]}...\")\n",
    "            continue\n",
    "        node = TreeNode(substring=split, position=indice, name=name, size=len(split))\n",
    "        root_nodes.append(node)\n",
    "\n",
    "    # Função recursiva para dividir nós\n",
    "    def divide_node(node: TreeNode, text: str):\n",
    "        if node.size <= min_size:\n",
    "            return  # Nó é uma folha\n",
    "\n",
    "        # Calcular pontos de divisão\n",
    "        mid = node.size // 2\n",
    "        left_substring = node.substring[:mid]\n",
    "        right_substring = node.substring[mid:]\n",
    "\n",
    "        # Encontrar posições das substrings no texto\n",
    "        left_indice = text.find(left_substring, node.position)\n",
    "        if left_indice == -1:\n",
    "            print(f\"Subsplit esquerdo não encontrado: {left_substring[:30]}...\")\n",
    "            return\n",
    "        right_indice = text.find(right_substring, left_indice + len(left_substring))\n",
    "        if right_indice == -1:\n",
    "            print(f\"Subsplit direito não encontrado: {right_substring[:30]}...\")\n",
    "            return\n",
    "\n",
    "        # Criar nós filhos\n",
    "        left_node = TreeNode(substring=left_substring, position=left_indice, name=name, size=len(left_substring))\n",
    "        right_node = TreeNode(substring=right_substring, position=right_indice, name=name, size=len(right_substring))\n",
    "        node.add_children(left_node, right_node)\n",
    "\n",
    "        # Recursivamente dividir os filhos\n",
    "        divide_node(left_node, text)\n",
    "        divide_node(right_node, text)\n",
    "\n",
    "    # Dividir cada nó raiz recursivamente\n",
    "    for root in root_nodes:\n",
    "        divide_node(root, text)\n",
    "\n",
    "    return root_nodes  # Retorna a lista de nós raiz da árvore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def signed_log_transform(y):\n",
    "    return np.sign(y) * np.log1p(np.abs(y))\n",
    "\n",
    "def inverse_signed_log_transform(y_transformed):\n",
    "    return np.sign(y_transformed) * (np.expm1(np.abs(y_transformed)))\n",
    "\n",
    "def dissimilaridade(S):\n",
    "\tepsilon = 0\n",
    "\tif S == 0:\n",
    "\t\tmax_x = np.log(np.finfo(np.float32).max)\n",
    "\t\tepsilon = (1/(max_x-1))\n",
    "        \n",
    "\treturn (1 - (S+epsilon)) / (S+epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from collections import deque\n",
    "from typing import Optional, List\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to the databases\n",
    "db1_path = './fineweb'\n",
    "db2_path = 'books.duckdb'\n",
    "batch_size = 150\n",
    "\n",
    "class IterableSequenceDataset(IterableDataset):\n",
    "\tdef __init__(self, path, batch_size=150):\n",
    "\t\tsuper(IterableSequenceDataset, self).__init__()\n",
    "\t\tself.fw = load_dataset(path, streaming=True)\n",
    "\t\tself.counter = 0\n",
    "\t\tself.batch_size = batch_size\n",
    "\n",
    "\tdef _calculate_distances_level_order(root: Optional[TreeNode]) -> List[int]:\n",
    "\t\tif not root:\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\tqueue = deque([root])\n",
    "\t\tdf = pd.DataFrame(columns=['embeddings','lengths','targets'])\n",
    "\n",
    "\t\tdistances = []\n",
    "\n",
    "\t\twhile queue:\n",
    "\t\t\tcurrent = queue.popleft()\n",
    "\t\t\t\n",
    "\t\t\t# Verifica se ambos os filhos existem\n",
    "\t\t\tif current.left and current.right:\n",
    "\t\t\t\tdistance = signed_log_transform(current.left.position - current.right.position)\n",
    "\t\t\t\t\n",
    "\t\t\t\tnova_linha = {'embeddings': embedding, 'lengths': length, 'targets': target}\n",
    "\t\t\t\tdf = df.append(nova_linha, ignore_index=True)\n",
    "\t\t\t\t\n",
    "\t\t\t\tdistances.append(distance)\n",
    "\n",
    "\t\t\t# Adiciona os filhos à fila para continuar a travessia\n",
    "\t\t\tif current.left:\n",
    "\t\t\t\tqueue.append(current.left)\n",
    "\t\t\tif current.right:\n",
    "\t\t\t\tqueue.append(current.right)\n",
    "\t\t\n",
    "\t\treturn distances\n",
    "\n",
    "\tdef _fetch_data(self, index):\n",
    "\t\tdf = []\n",
    "\t\tfor sample in tqdm(self.fw['train'], desc=\"fineweb\"):\n",
    "\t\t\tif self.counter % self.batch_size == 0:\n",
    "\t\t\t\tdf['target_norm'] = scaler.transform(df[['target_transformed']])\n",
    "\t\t\t\treturn df\n",
    "\t\t\t\n",
    "\t\t\ttext = cleartext(sample['text'])\n",
    "\t\t\tname = sample['url']\n",
    "\t\t\tnodes_by_size = splitter_length_tree_recursive(text, name,max_size=int(len(text)/2), min_size=64, use_tqdm=True)\n",
    "\n",
    "\t\t\tbreak\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Defina o número de elementos originais em cada coluna\n",
    "\t\tnum_content1 = len(df['content1'])\n",
    "\n",
    "\t\t# Combinar as duas colunas de conteúdo em uma lista\n",
    "\t\tcontents = list(df['content1']) + list(df['content2'])\n",
    "\n",
    "\t\t# Tokenização combinada\n",
    "\t\tencoding = embedding_generator.tokenizer(\n",
    "\t\t\tcontents,\n",
    "\t\t\treturn_tensors=\"pt\",\n",
    "\t\t\tpadding=True,  # Padding dinâmico\n",
    "\t\t\ttruncation=True\n",
    "\t\t)\n",
    "\n",
    "\t\t# Separar os encodings de volta para cada conjunto\n",
    "\t\tencoding1 = {key: encoding[key][:num_content1] for key in encoding.keys()}\n",
    "\t\tencoding2 = {key: encoding[key][num_content1:] for key in encoding.keys()}\n",
    "\n",
    "\t\t# Converter para o dispositivo\n",
    "\t\tinput_ids1 = encoding1['input_ids'].to(embedding_generator.device)\n",
    "\t\tattention_mask1 = encoding1['attention_mask'].to(embedding_generator.device)\n",
    "\t\tinput_ids2 = encoding2['input_ids'].to(embedding_generator.device)\n",
    "\t\tattention_mask2 = encoding2['attention_mask'].to(embedding_generator.device)\n",
    "\n",
    "\t\t# Obter embeddings\n",
    "\t\tembeddings1 = embedding_generator.weights[input_ids1]\n",
    "\t\tembeddings2 = embedding_generator.weights[input_ids2]\n",
    "\n",
    "\t\t# Concatenar as duas sequências no eixo 1\n",
    "\t\tembeddings = torch.stack([embeddings1, embeddings2], dim=1)  # (batch_size, 2, seq_length, vector_size)\n",
    "\n",
    "\t\t# Calcular comprimentos\n",
    "\t\tlengths1 = attention_mask1.sum(dim=1)\n",
    "\t\tlengths2 = attention_mask2.sum(dim=1)\n",
    "\t\tlengths = torch.stack([lengths1, lengths2], dim=1)  # (batch_size, 2)\n",
    "\n",
    "\t\t# Converter targets\n",
    "\t\ttargets = torch.tensor(df['target_norm'], dtype=torch.float16)  # (batch_size,)\n",
    "\n",
    "\t\treturn embeddings,lengths,targets\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tIterator que gera dados continuamente.\n",
    "\n",
    "\t\tYields:\n",
    "\t\t\ttuple: (content1, content2, target) para cada amostra.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\twhile True:\n",
    "\t\t\t\tembeddings,lengths,targets = self._fetch_data(conn)\n",
    "\t\t\t\tfor i in range(batch_size): \n",
    "\t\t\t\t\tyield embeddings[i], lengths[i], targets[i]\n",
    "\t\tfinally:\n",
    "\t\t\t# Garantir que a conexão seja fechada quando o iterador for finalizado\n",
    "\t\t\tconn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_transformed</th>\n",
       "      <th>content1</th>\n",
       "      <th>content2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>A tempestade durou várias horas durante a noite.</td>\n",
       "      <td>A tempestade durou várias horas durante a noite.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Ele estudou intensamente para o exame.</td>\n",
       "      <td>Ele estudou intensamente para o exame.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>A fábrica reduziu a emissão de poluentes.</td>\n",
       "      <td>A fábrica reduziu a emissão de poluentes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Maria começou a praticar exercícios regularmente.</td>\n",
       "      <td>Maria começou a praticar exercícios regularmente.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>O motor do carro parou de funcionar.</td>\n",
       "      <td>O motor do carro parou de funcionar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-1.718282</td>\n",
       "      <td>Isso resultou na redução significativa das emi...</td>\n",
       "      <td>Após anos de pesquisa e desenvolvimento, a equ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>-1.718282</td>\n",
       "      <td>Ela sentiu fome e falta de energia durante a m...</td>\n",
       "      <td>O café da manhã foi esquecido.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>-1.718282</td>\n",
       "      <td>As praias ficaram mais limpas e atraentes para...</td>\n",
       "      <td>A iniciativa comunitária organizou mutirões de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>-1.718282</td>\n",
       "      <td>Finalmente recebeu uma oferta de trabalho em u...</td>\n",
       "      <td>Ele atualizou seu currículo e participou de vá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>-1.718282</td>\n",
       "      <td>Suas peças ganharam destaque em exposições int...</td>\n",
       "      <td>O artista decidiu experimentar novas técnicas ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target_transformed                                           content1  \\\n",
       "0              0.000000   A tempestade durou várias horas durante a noite.   \n",
       "1              0.000000             Ele estudou intensamente para o exame.   \n",
       "2              0.000000          A fábrica reduziu a emissão de poluentes.   \n",
       "3              0.000000  Maria começou a praticar exercícios regularmente.   \n",
       "4              0.000000               O motor do carro parou de funcionar.   \n",
       "..                  ...                                                ...   \n",
       "145           -1.718282  Isso resultou na redução significativa das emi...   \n",
       "146           -1.718282  Ela sentiu fome e falta de energia durante a m...   \n",
       "147           -1.718282  As praias ficaram mais limpas e atraentes para...   \n",
       "148           -1.718282  Finalmente recebeu uma oferta de trabalho em u...   \n",
       "149           -1.718282  Suas peças ganharam destaque em exposições int...   \n",
       "\n",
       "                                              content2  \n",
       "0     A tempestade durou várias horas durante a noite.  \n",
       "1               Ele estudou intensamente para o exame.  \n",
       "2            A fábrica reduziu a emissão de poluentes.  \n",
       "3    Maria começou a praticar exercícios regularmente.  \n",
       "4                 O motor do carro parou de funcionar.  \n",
       "..                                                 ...  \n",
       "145  Após anos de pesquisa e desenvolvimento, a equ...  \n",
       "146                     O café da manhã foi esquecido.  \n",
       "147  A iniciativa comunitária organizou mutirões de...  \n",
       "148  Ele atualizou seu currículo e participou de vá...  \n",
       "149  O artista decidiu experimentar novas técnicas ...  \n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'target_transformed': [inverse_signed_log_transform(1)]*50,\n",
    "    \"content1\": [\"A tempestade durou várias horas durante a noite.\", \"Ele estudou intensamente para o exame.\", \"A fábrica reduziu a emissão de poluentes.\", \"Maria começou a praticar exercícios regularmente.\", \"O motor do carro parou de funcionar.\", \"Houve uma forte seca na região.\", \"Ela começou a dormir melhor.\", \"A escola adotou uma alimentação saudável.\", \"O projeto teve apoio governamental.\", \"A internet caiu durante a reunião.\", \"Ele começou a economizar dinheiro mensalmente.\", \"A estrada estava em péssimo estado de conservação.\", \"O sistema de ar condicionado foi desligado no escritório.\", \"A empresa investiu em marketing digital.\", \"Ana não revisou o relatório antes de enviar.\", \"As crianças brincaram no parque até tarde.\", \"O curso de capacitação foi oferecido aos funcionários.\", \"Ele comprou um celular novo com câmera de alta qualidade.\", \"A reforma no prédio foi concluída.\", \"A região teve um crescimento populacional rápido.\", \"Carla começou a meditar diariamente.\", \"O atleta intensificou seu treino antes da competição.\", \"O sinal de celular na região foi ampliado.\", \"As temperaturas caíram drasticamente durante o inverno.\", \"Ele não configurou o alarme antes de dormir.\", \"A empresa ofereceu benefícios extras para seus funcionários.\", \"Houve um vazamento de gás na cozinha do restaurante.\", \"A biblioteca da escola foi modernizada com novos livros e tecnologia.\", \"Marcos passou a fazer pausas regulares durante o expediente.\", \"A empresa desenvolveu um aplicativo intuitivo para clientes.\", \"Devido ao aumento das chuvas nas últimas semanas, o nível dos rios subiu rapidamente e ultrapassou a capacidade das barragens.\", \"Ela decidiu começar uma rotina de alimentação balanceada, exercícios físicos regulares e meditação diária para melhorar sua saúde física e mental.\", \"A empresa, que sofria com baixos índices de produtividade, implementou uma nova estratégia de gestão focada no desenvolvimento dos funcionários e na cultura organizacional.\", \"O governo lançou um programa nacional de reciclagem e incentivou a participação ativa dos cidadãos por meio de campanhas de conscientização em escolas, empresas e residências.\", \"Após uma longa estiagem que afetou grande parte do território agrícola do país, o governo implementou um pacote emergencial de apoio aos agricultores, incluindo subsídios e incentivos para a recuperação das lavouras.\", \"Ele deixou o celular carregando a noite inteira sem usar carregadores de segurança e em um local sem ventilação.\", \"A comunidade local se uniu para limpar e revitalizar a praça abandonada, que estava sem manutenção há anos e havia se tornado um ponto de descarte irregular de lixo.\", \"Durante a reforma do prédio, descobriu-se que a estrutura tinha falhas graves, e o prazo para conclusão foi ampliado em seis meses para garantir a segurança.\", \"Um furacão atingiu a região costeira do país, com ventos de mais de 200 km/h, causando destruição em várias cidades e deixando milhares de pessoas desabrigadas.\", \"Ela esqueceu de regar a planta de sua varanda durante o mês todo, e o clima estava seco.\", \"João quebrou o braço jogando futebol.\", \"Devido a uma combinação de fatores climáticos extremos, incluindo ventos fortes e chuvas intensas, a região sofreu graves danos estruturais e interrupção no fornecimento de energia elétrica por vários dias.\", \"Ela esqueceu de levar o guarda-chuva.\", \"A empresa adotou práticas sustentáveis em sua produção, reduzindo o uso de plástico e implementando programas de reciclagem.\", \"A criança se recusou a comer legumes durante toda a semana.\", \"Após anos de pesquisa e desenvolvimento, a equipe científica finalmente descobriu um método eficiente para produzir energia limpa a partir de fontes renováveis.\", \"O café da manhã foi esquecido.\", \"A iniciativa comunitária organizou mutirões de limpeza nas praias locais regularmente durante o verão.\", \"Ele atualizou seu currículo e participou de várias entrevistas de emprego nos últimos meses.\", \"O artista decidiu experimentar novas técnicas em suas pinturas, incorporando elementos digitais e materiais reciclados em suas obras.\"],\n",
    "    \"content2\": [\"Pela manhã, muitas ruas estavam alagadas.\", \"Conseguiu uma nota alta na prova.\", \"A qualidade do ar na cidade melhorou.\", \"Ela perdeu peso e aumentou sua energia.\", \"Ele teve que chamar o guincho para levar o carro à oficina.\", \"As plantações foram prejudicadas, e a colheita foi menor.\", \"Sua disposição durante o dia melhorou significativamente.\", \"Os alunos passaram a ter mais energia e melhor concentração.\", \"Conseguiu concluir as fases iniciais rapidamente.\", \"A comunicação com a equipe foi interrompida.\", \"Conseguiu juntar uma quantia para uma viagem.\", \"O trânsito ficou mais lento e perigoso para os motoristas.\", \"O ambiente ficou quente e desconfortável para os funcionários.\", \"As vendas aumentaram significativamente.\", \"O documento continha erros e precisou ser corrigido.\", \"Elas ficaram cansadas e dormiram rapidamente ao chegar em casa.\", \"Eles melhoraram suas habilidades e eficiência no trabalho.\", \"Passou a tirar fotos mais nítidas e de melhor resolução.\", \"O local ficou mais seguro e esteticamente agradável.\", \"A demanda por moradias e serviços aumentou consideravelmente.\", \"Ela sentiu uma melhora no seu foco e redução do estresse.\", \"Teve um melhor desempenho e conquistou o primeiro lugar.\", \"A conexão ficou mais estável e acessível para os moradores.\", \"A procura por agasalhos e cobertores aumentou nas lojas.\", \"Acabou se atrasando para o trabalho na manhã seguinte.\", \"A satisfação e motivação dos colaboradores aumentaram.\", \"O local foi evacuado por segurança, e o serviço foi interrompido temporariamente.\", \"Os alunos começaram a frequentá-la mais e a melhorar seu desempenho acadêmico.\", \"Ele se sentiu mais produtivo e menos cansado ao final do dia.\", \"O número de usuários aumentou rapidamente.\", \"Diversas áreas urbanas e rurais foram afetadas por inundações, forçando muitas famílias a deixarem suas casas temporariamente.\", \"Em poucos meses, notou uma grande melhora em sua disposição, concentração e níveis de energia, além de perder peso.\", \"Em menos de um ano, a moral da equipe melhorou, a rotatividade diminuiu, e a produtividade geral aumentou em cerca de 30%.\", \"Como resultado, houve uma redução significativa na quantidade de resíduos sólidos em aterros e uma maior economia de recursos naturais.\", \"Em um ano, a produção agrícola voltou a níveis estáveis, e o impacto econômico negativo foi mitigado, beneficiando a população.\", \"Pela manhã, o dispositivo estava superaquecido e apresentou danos permanentes na bateria, reduzindo sua capacidade de funcionamento.\", \"Com a revitalização, a praça voltou a ser um local de encontro para moradores, e a segurança da área também melhorou.\", \"Apesar do atraso, a reforma resultou em um edifício mais seguro, confortável e com um aumento significativo no valor do imóvel.\", \"A resposta emergencial foi mobilizada rapidamente, com abrigos temporários e ajuda humanitária distribuída para minimizar o impacto nas vítimas.\", \"A planta murchou completamente e, infelizmente, não conseguiu ser recuperada, tendo que ser substituída.\", \"Ele ficou impossibilitado de trabalhar por duas semanas.\", \"A comunidade teve que recorrer a abrigos temporários, e a economia local sofreu uma queda significativa devido à paralisação das atividades comerciais.\", \"Ficou molhada durante o trajeto para o trabalho.\", \"A reputação da empresa melhorou, atraindo consumidores conscientes e aumentando as vendas em 20%.\", \"Sua mãe ficou preocupada com a falta de nutrientes na alimentação dele e decidiu consultar um nutricionista.\", \"Isso resultou na redução significativa das emissões de carbono e no avanço tecnológico sustentável, beneficiando o meio ambiente globalmente.\", \"Ela sentiu fome e falta de energia durante a manhã de trabalho.\", \"As praias ficaram mais limpas e atraentes para turistas, além de promover a conscientização ambiental entre os moradores.\", \"Finalmente recebeu uma oferta de trabalho em uma empresa renomada, melhorando sua estabilidade financeira.\", \"Suas peças ganharam destaque em exposições internacionais, ampliando seu reconhecimento e alcance no mercado de arte.\"]\n",
    "}\n",
    "\n",
    "# Crie o DataFrame\n",
    "df_validation = pd.DataFrame(data)\n",
    "\n",
    "# Adicionar frases iguais\n",
    "df_zero = pd.DataFrame({\n",
    "    'target_transformed': [0] * len(data['target_transformed']),\n",
    "    'content1': df_validation['content1'],\n",
    "    'content2': df_validation['content1']\n",
    "})\n",
    "\n",
    "# Adicionar frases com a ordem contrária\n",
    "df_reversed = pd.DataFrame({\n",
    "    'target_transformed': [inverse_signed_log_transform(-1)] * len(data['target_transformed']),\n",
    "    'content1': df_validation['content2'],\n",
    "    'content2': df_validation['content1']\n",
    "})\n",
    "\n",
    "# Concatenar os dois DataFrames\n",
    "df_validation = pd.concat([df_zero, df_validation, df_reversed], ignore_index=True)\n",
    "\n",
    "df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    embeddings, lengths, targets = zip(*batch)\n",
    "    return torch.stack(embeddings).to(torch.float32), torch.stack(lengths), torch.stack(targets).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = IterableSequenceDataset(db1_path, db2_path)\n",
    "train_dataloader = DataLoader(\n",
    "  training_dataset, \n",
    "  batch_size=batch_size, \n",
    "  num_workers=0,\n",
    "  collate_fn=collate_fn,\n",
    "  pin_memory=False,\n",
    "  shuffle=False,\n",
    "  #persistent_workers=True\n",
    ")\n",
    "\n",
    "validation_dataset = SequenceDataset_val(df_validation)\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=150,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 2, 231, 2048])\n"
     ]
    }
   ],
   "source": [
    "#  Converter DataLoader para iterador\n",
    "data_iter = iter(train_dataloader)\n",
    "# Pegar o primeiro lote\n",
    "embeddings, lengths, targets = next(data_iter)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class SequenceToTwoModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(SequenceToTwoModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM para processar as sequências\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Camadas totalmente conectadas após concatenar as saídas do LSTM para ambas as sequências\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 1)  # hidden_size para cada sequência\n",
    "\n",
    "    def forward(self, sequences, lengths):\n",
    "        \"\"\"\n",
    "        Passagem para frente do modelo.\n",
    "\n",
    "        Args:\n",
    "            sequences (Tensor): Tensor de embeddings de forma (batch_size, 2, seq_len, input_size)\n",
    "            lengths (Tensor): Tensor de comprimentos de forma (batch_size, 2)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Previsões do modelo de forma (batch_size,)\n",
    "        \"\"\"\n",
    "        batch_size = sequences.size(0)\n",
    "        seq_len = sequences.size(2)\n",
    "        input_size = sequences.size(3)\n",
    "\n",
    "        # Combinar as duas sequências ao longo da dimensão do batch\n",
    "        combined_sequences = sequences.view(batch_size * 2, seq_len, input_size)\n",
    "        combined_lengths = lengths.view(batch_size * 2)\n",
    "\n",
    "        # Ordenar as sequências de acordo com seus comprimentos (necessário para pack_padded_sequence)\n",
    "        lengths_sorted, sorted_idx = combined_lengths.sort(0, descending=True)\n",
    "        sequences_sorted = combined_sequences[sorted_idx]\n",
    "\n",
    "        # Empacotar as sequências para ignorar os paddings\n",
    "        packed_seq = pack_padded_sequence(sequences_sorted, lengths_sorted.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        # Passar as sequências empacotadas pelo LSTM\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_seq)\n",
    "\n",
    "        # Obter o último estado oculto de cada sequência\n",
    "        last_hidden = h_n[-1]  # (batch_size * 2, hidden_size)\n",
    "\n",
    "        # Desempacotar a sequência e restaurar a ordem original\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        last_hidden = last_hidden[original_idx]\n",
    "\n",
    "        # Reorganizar a saída para separar as sequências\n",
    "        lstm_out = last_hidden.view(batch_size, 2, self.hidden_size)\n",
    "\n",
    "        # Concatenar as saídas das duas sequências\n",
    "        combined = lstm_out.view(batch_size, -1)  # (batch_size, hidden_size * 2)\n",
    "\n",
    "        # Passar pelas camadas totalmente conectadas\n",
    "        out = self.fc1(combined)\n",
    "\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "limit_train_batches=10\n",
    "max_epochs=50\n",
    "\n",
    "class CustomHuberLoss(nn.Module):\n",
    "\tdef __init__(self, delta=1.0):\n",
    "\t\tsuper(CustomHuberLoss, self).__init__()\n",
    "\t\tself.delta = delta\n",
    "\t\tself.huber_loss = nn.HuberLoss(delta=delta)\n",
    "\n",
    "\tdef forward(self, output, target):\n",
    "\t\t# Verificar os sinais de output e target usando os limites fornecidos\n",
    "\t\toutput_sign = (output <= negative_threshold).float() - (output >= positive_threshold).float()\n",
    "\t\ttarget_sign = (target <= negative_threshold).float() - (target >= positive_threshold).float()\n",
    "\n",
    "\t\t# Calcular a perda Huber normal\n",
    "\t\tloss = self.huber_loss(output, target)\n",
    "\n",
    "\t\t# Verificar se o sinal de output é diferente de target\n",
    "\t\tsign_diff = (output_sign != target_sign).float()\n",
    "\n",
    "\t\t# Se os sinais forem diferentes, multiplicar a perda por 2\n",
    "\t\tadjusted_loss = loss * (1 + sign_diff)  # Duas vezes a perda se os sinais forem diferentes\n",
    "\n",
    "\t\treturn adjusted_loss.mean()\n",
    "\n",
    "class SequenceToTwoLightning(pl.LightningModule):\n",
    "\tdef __init__(self, input_size=2048, hidden_size=128, num_layers=1, lr=1e-3, max_lr=1e-2):\n",
    "\t\tsuper(SequenceToTwoLightning, self).__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\t# Instanciar o modelo modificado\n",
    "\t\tself.model = SequenceToTwoModel(\n",
    "\t\t\tinput_size=input_size,\n",
    "\t\t\thidden_size=hidden_size,\n",
    "\t\t\tnum_layers=num_layers\n",
    "\t\t)\n",
    "\n",
    "\t\t# Função de perda\n",
    "\t\tself.criterion = CustomHuberLoss()\n",
    "\n",
    "\tdef forward(self, sequences, lengths):\n",
    "\t\treturn self.model(sequences, lengths)\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\tembeddings, lengths, targets = batch  # embeddings: (batch_size, 2, seq_len, vector_size)\n",
    "\t\tembeddings = embeddings.to(self.device)\n",
    "\t\tlengths = lengths.to(self.device)\n",
    "\t\ttargets = targets.to(self.device)\n",
    "\n",
    "\t\t# Passagem para frente\n",
    "\t\toutputs = self(embeddings, lengths)  # (batch_size,)\n",
    "\n",
    "\t\t# Computar a perda\t\n",
    "\t\tloss = self.criterion(outputs, targets)\n",
    "\n",
    "\t\t# Logar a perda de treinamento\n",
    "\t\tself.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef validation_step(self, batch, batch_idx):\n",
    "\t\tembeddings, lengths, targets = batch\n",
    "\t\tembeddings = embeddings.to(self.device)\n",
    "\t\tlengths = lengths.to(self.device)\n",
    "\t\ttargets = targets.to(self.device)\n",
    "\n",
    "\t\t# Passagem para frente\n",
    "\t\toutputs = self(embeddings, lengths)\n",
    "\n",
    "\t\t# Computar a perda\n",
    "\t\tloss = self.criterion(outputs, targets)\n",
    "\n",
    "\t\t# Logar a perda de validação\n",
    "\t\tself.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\t# Otimizador Adam\n",
    "\t\toptimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "\t\t# Configuração do OneCycleLR\n",
    "\t\tscheduler = OneCycleLR(\n",
    "\t\t\toptimizer,\n",
    "\t\t\tmax_lr=self.hparams.max_lr,   # A maior taxa de aprendizado (geralmente 1/10 do valor inicial)\n",
    "\t\t\ttotal_steps=(max_epochs*limit_train_batches),  # Total de passos de treinamento (batched * epochs)\n",
    "\t\t\tanneal_strategy='cos',  # Estratégia de decaimento (coseno ou linear)\n",
    "\t\t\tpct_start=0.3,  # Proporção de passos em que a taxa de aprendizado vai aumentar\n",
    "\t\t\tcycle_momentum=True,  # Ajuste também o momento durante o ciclo\n",
    "\t\t\tbase_momentum=0.85,  # Momento inicial\n",
    "\t\t\tmax_momentum=0.95,   # Momento máximo\n",
    "\t\t)\n",
    "\n",
    "\t\t# Retornar otimizador e scheduler\n",
    "\t\treturn {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Instanciando o TensorBoardLogger\n",
    "tensorboard_logger = TensorBoardLogger('tb_logs', name='model_name')\n",
    "\n",
    "model = SequenceToTwoLightning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jadson/anaconda3/envs/pytorch/lib/python3.12/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/jadson/anaconda3/envs/pytorch/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/jadson/Documentos/Tempo-nas-Narrativas/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | SequenceToTwoModel | 1.1 M  | train\n",
      "1 | criterion | CustomHuberLoss    | 0      | train\n",
      "---------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.462     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3ca5b0841c483494cf0933789524e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jadson/anaconda3/envs/pytorch/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/jadson/anaconda3/envs/pytorch/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/jadson/anaconda3/envs/pytorch/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092d0d38e01d421caa1bcc4d34644a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d415fd99701444e8ae7d1f90749c1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c71382dcd3a40b5a981e1c826bac7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e65c5056f84af3beea16447388acf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb81d643ad044c88b92382d1d47f6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dfbce36f04412d8cdc8b48600097ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eded5678bb44f84bb1c4e1f0e36e502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3a089bd1a64d5ca16ad6676040c2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9524dc7cfd4668a6407f8205a28b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4158116859c3462190453250a2a1df92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91bd0af517648e8ae1507f75ef25c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fe61df95da4a858a14d00b6921a605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e91e1b1a1e4269987cf7084f4ebcac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39934b70ddf748bd9fe4756338a99844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f18105c0b1449cad5291f4aaf1651d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d28e0fc94242eabe3f8a2d537ba562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Definir callbacks, por exemplo, ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    precision=16,\n",
    "    accelerator='auto',\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accumulate_grad_batches=10,\n",
    "    deterministic=False,\n",
    "    limit_train_batches=limit_train_batches,\n",
    "    profiler=\"simple\",\n",
    "    logger=tensorboard_logger\n",
    ")\n",
    "\n",
    "# Iniciar o treinamento\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader, #IterableDataset\n",
    "    val_dataloaders=validation_dataloader #Dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_dataloader torch.Size([150, 2, 61, 2048]) torch.Size([150, 2]) torch.Size([150])\n",
      "tensor(0.3333)\n",
      "Resultado da previsão para o par 1: 0.3219577670097351\n",
      "Resultado da previsão para o par 2: 0.3220708966255188\n",
      "Resultado da previsão para o par 3: 0.32194820046424866\n",
      "Resultado da previsão para o par 4: 0.3219943940639496\n",
      "Resultado da previsão para o par 5: 0.3220708966255188\n",
      "Resultado da previsão para o par 6: 0.3220708966255188\n",
      "Resultado da previsão para o par 7: 0.3221301734447479\n",
      "Resultado da previsão para o par 8: 0.32197245955467224\n",
      "Resultado da previsão para o par 9: 0.3221301734447479\n",
      "Resultado da previsão para o par 10: 0.3220262825489044\n",
      "Resultado da previsão para o par 11: 0.3220262825489044\n",
      "Resultado da previsão para o par 12: 0.32197245955467224\n",
      "Resultado da previsão para o par 13: 0.32194820046424866\n",
      "Resultado da previsão para o par 14: 0.3222029507160187\n",
      "Resultado da previsão para o par 15: 0.3220262825489044\n",
      "Resultado da previsão para o par 16: 0.3219943940639496\n",
      "Resultado da previsão para o par 17: 0.32197245955467224\n",
      "Resultado da previsão para o par 18: 0.3219577670097351\n",
      "Resultado da previsão para o par 19: 0.3219943940639496\n",
      "Resultado da previsão para o par 20: 0.3219943940639496\n",
      "Resultado da previsão para o par 21: 0.3220262825489044\n",
      "Resultado da previsão para o par 22: 0.3219577670097351\n",
      "Resultado da previsão para o par 23: 0.3219943940639496\n",
      "Resultado da previsão para o par 24: 0.3219577670097351\n",
      "Resultado da previsão para o par 25: 0.3219943940639496\n",
      "Resultado da previsão para o par 26: 0.32197245955467224\n",
      "Resultado da previsão para o par 27: 0.32194820046424866\n",
      "Resultado da previsão para o par 28: 0.3219360113143921\n",
      "Resultado da previsão para o par 29: 0.32194212079048157\n",
      "Resultado da previsão para o par 30: 0.32197245955467224\n",
      "Resultado da previsão para o par 31: 0.33699411153793335\n",
      "Resultado da previsão para o par 32: 0.2754436433315277\n",
      "Resultado da previsão para o par 33: 0.32705819606781006\n",
      "Resultado da previsão para o par 34: 0.42726361751556396\n",
      "Resultado da previsão para o par 35: 0.29133927822113037\n",
      "Resultado da previsão para o par 36: 0.33539319038391113\n",
      "Resultado da previsão para o par 37: 0.35122257471084595\n",
      "Resultado da previsão para o par 38: 0.2759194076061249\n",
      "Resultado da previsão para o par 39: 0.30623504519462585\n",
      "Resultado da previsão para o par 40: 0.32193291187286377\n",
      "Resultado da previsão para o par 41: 0.3219577670097351\n",
      "Resultado da previsão para o par 42: 0.3330390751361847\n",
      "Resultado da previsão para o par 43: 0.3219577670097351\n",
      "Resultado da previsão para o par 44: 0.33817118406295776\n",
      "Resultado da previsão para o par 45: 0.32194212079048157\n",
      "Resultado da previsão para o par 46: 0.2761746644973755\n",
      "Resultado da previsão para o par 47: 0.3220262825489044\n",
      "Resultado da previsão para o par 48: 0.32193291187286377\n",
      "Resultado da previsão para o par 49: 0.32193318009376526\n",
      "Resultado da previsão para o par 50: 0.22087985277175903\n",
      "Resultado da previsão para o par 51: 0.3219362199306488\n",
      "Resultado da previsão para o par 52: 0.3220708966255188\n",
      "Resultado da previsão para o par 53: 0.32206740975379944\n",
      "Resultado da previsão para o par 54: 0.3219943940639496\n",
      "Resultado da previsão para o par 55: 0.3219574987888336\n",
      "Resultado da previsão para o par 56: 0.3219667971134186\n",
      "Resultado da previsão para o par 57: 0.32203561067581177\n",
      "Resultado da previsão para o par 58: 0.32195791602134705\n",
      "Resultado da previsão para o par 59: 0.32201093435287476\n",
      "Resultado da previsão para o par 60: 0.32200121879577637\n",
      "Resultado da previsão para o par 61: 0.32195672392845154\n",
      "Resultado da previsão para o par 62: 0.3219316303730011\n",
      "Resultado da previsão para o par 63: 0.32194820046424866\n",
      "Resultado da previsão para o par 64: 0.3222062289714813\n",
      "Resultado da previsão para o par 65: 0.32196688652038574\n",
      "Resultado da previsão para o par 66: 0.32193851470947266\n",
      "Resultado da previsão para o par 67: 0.32195791602134705\n",
      "Resultado da previsão para o par 68: 0.3219362199306488\n",
      "Resultado da previsão para o par 69: 0.32196006178855896\n",
      "Resultado da previsão para o par 70: 0.32194989919662476\n",
      "Resultado da previsão para o par 71: 0.32194244861602783\n",
      "Resultado da previsão para o par 72: 0.32194074988365173\n",
      "Resultado da previsão para o par 73: 0.3219430446624756\n",
      "Resultado da previsão para o par 74: 0.3219362199306488\n",
      "Resultado da previsão para o par 75: 0.32193851470947266\n",
      "Resultado da previsão para o par 76: 0.32194775342941284\n",
      "Resultado da previsão para o par 77: 0.3219296932220459\n",
      "Resultado da previsão para o par 78: 0.32193177938461304\n",
      "Resultado da previsão para o par 79: 0.32194212079048157\n",
      "Resultado da previsão para o par 80: 0.3220450282096863\n",
      "Resultado da previsão para o par 81: 0.3513651490211487\n",
      "Resultado da previsão para o par 82: 0.29484808444976807\n",
      "Resultado da previsão para o par 83: 0.4216400384902954\n",
      "Resultado da previsão para o par 84: 0.3474341928958893\n",
      "Resultado da previsão para o par 85: 0.3027665615081787\n",
      "Resultado da previsão para o par 86: 0.32549822330474854\n",
      "Resultado da previsão para o par 87: 0.35200971364974976\n",
      "Resultado da previsão para o par 88: 0.2670556902885437\n",
      "Resultado da previsão para o par 89: 0.30911698937416077\n",
      "Resultado da previsão para o par 90: 0.32193291187286377\n",
      "Resultado da previsão para o par 91: 0.3219723105430603\n",
      "Resultado da previsão para o par 92: 0.26763731241226196\n",
      "Resultado da previsão para o par 93: 0.3219577670097351\n",
      "Resultado da previsão para o par 94: 0.3179205358028412\n",
      "Resultado da previsão para o par 95: 0.32192978262901306\n",
      "Resultado da previsão para o par 96: 0.27471980452537537\n",
      "Resultado da previsão para o par 97: 0.32194244861602783\n",
      "Resultado da previsão para o par 98: 0.3484010398387909\n",
      "Resultado da previsão para o par 99: 0.3219328820705414\n",
      "Resultado da previsão para o par 100: 0.2961689233779907\n",
      "Resultado da previsão para o par 101: 0.32195985317230225\n",
      "Resultado da previsão para o par 102: 0.3220708966255188\n",
      "Resultado da previsão para o par 103: 0.32201093435287476\n",
      "Resultado da previsão para o par 104: 0.3219943940639496\n",
      "Resultado da previsão para o par 105: 0.32204803824424744\n",
      "Resultado da previsão para o par 106: 0.3220461905002594\n",
      "Resultado da previsão para o par 107: 0.322066992521286\n",
      "Resultado da previsão para o par 108: 0.3219723105430603\n",
      "Resultado da previsão para o par 109: 0.32206740975379944\n",
      "Resultado da previsão para o par 110: 0.32201945781707764\n",
      "Resultado da previsão para o par 111: 0.32201775908470154\n",
      "Resultado da previsão para o par 112: 0.3219754993915558\n",
      "Resultado da previsão para o par 113: 0.32194820046424866\n",
      "Resultado da previsão para o par 114: 0.3221268951892853\n",
      "Resultado da previsão para o par 115: 0.3220171630382538\n",
      "Resultado da previsão para o par 116: 0.3219941556453705\n",
      "Resultado da previsão para o par 117: 0.3219723105430603\n",
      "Resultado da previsão para o par 118: 0.32195985317230225\n",
      "Resultado da previsão para o par 119: 0.32199209928512573\n",
      "Resultado da previsão para o par 120: 0.3219926953315735\n",
      "Resultado da previsão para o par 121: 0.32201987504959106\n",
      "Resultado da previsão para o par 122: 0.32195910811424255\n",
      "Resultado da previsão para o par 123: 0.3219934403896332\n",
      "Resultado da previsão para o par 124: 0.32195985317230225\n",
      "Resultado da previsão para o par 125: 0.3219941556453705\n",
      "Resultado da previsão para o par 126: 0.32197290658950806\n",
      "Resultado da previsão para o par 127: 0.32195165753364563\n",
      "Resultado da previsão para o par 128: 0.3219373822212219\n",
      "Resultado da previsão para o par 129: 0.32194212079048157\n",
      "Resultado da previsão para o par 130: 0.32199832797050476\n",
      "Resultado da previsão para o par 131: 0.2943774461746216\n",
      "Resultado da previsão para o par 132: 0.31598877906799316\n",
      "Resultado da previsão para o par 133: 0.27216383814811707\n",
      "Resultado da previsão para o par 134: 0.33709171414375305\n",
      "Resultado da previsão para o par 135: 0.329820454120636\n",
      "Resultado da previsão para o par 136: 0.37768855690956116\n",
      "Resultado da previsão para o par 137: 0.3118259012699127\n",
      "Resultado da previsão para o par 138: 0.33084776997566223\n",
      "Resultado da previsão para o par 139: 0.3281702995300293\n",
      "Resultado da previsão para o par 140: 0.32193291187286377\n",
      "Resultado da previsão para o par 141: 0.32195791602134705\n",
      "Resultado da previsão para o par 142: 0.4121139645576477\n",
      "Resultado da previsão para o par 143: 0.3219577670097351\n",
      "Resultado da previsão para o par 144: 0.34218358993530273\n",
      "Resultado da previsão para o par 145: 0.3219452500343323\n",
      "Resultado da previsão para o par 146: 0.28537389636039734\n",
      "Resultado da previsão para o par 147: 0.32201987504959106\n",
      "Resultado da previsão para o par 148: 0.3191005289554596\n",
      "Resultado da previsão para o par 149: 0.3219332993030548\n",
      "Resultado da previsão para o par 150: 0.19623024761676788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177078/1034891541.py:26: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  ((targets == -1) & (output < inverse_signed_log_transform(-1))) |  # Regra para -1\n",
      "/tmp/ipykernel_177078/1034891541.py:27: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  ((targets == 0) & ((inverse_signed_log_transform(-1) < output) & (output < inverse_signed_log_transform(1)))) |  # Regra para 0\n",
      "/tmp/ipykernel_177078/1034891541.py:28: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  ((targets == 1) & (inverse_signed_log_transform(1) < output))     # Regra para 1\n"
     ]
    }
   ],
   "source": [
    "#  Converter DataLoader para iterador\n",
    "data_iter = iter(validation_dataloader)\n",
    "# Pegar o primeiro lote\n",
    "embeddings, lengths, targets = next(data_iter)\n",
    "print(\"validation_dataloader\",embeddings.shape,lengths.shape,targets.shape)\n",
    "\n",
    "# Ensure your model and inputs are in float16 if using AMP\n",
    "#model = model.half()\n",
    "#embeddings = embeddings.half()\n",
    "#targets = targets.half()\n",
    "\n",
    "embeddings = embeddings.to(torch.float32)\n",
    "lengths = lengths.to(torch.int64) \n",
    "targets = targets.to(torch.float32)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Realizar a previsão\n",
    "with torch.no_grad():\n",
    "\toutput = model(embeddings, lengths)\n",
    "\toutput = output.numpy()\n",
    "\toutput = output.reshape(-1, 1)\n",
    "\toutput = scaler.inverse_transform(output)\n",
    "\n",
    "matches = (\n",
    "\t((targets == -1) & (output < negative_threshold)) |  # Regra para -1\n",
    "\t((targets == 0) & ((negative_threshold < output) & (output < positive_threshold))) |  # Regra para 0\n",
    "\t((targets == 1) & (positive_threshold < output))     # Regra para 1\n",
    ")\n",
    "\n",
    "print(matches.float().mean())\n",
    "\t\n",
    "# Imprimir os resultados da previsão\n",
    "for i, pred in enumerate(output):\n",
    "\t\tprint(f\"Resultado da previsão para o par {i+1}: {pred.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomHuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(CustomHuberLoss, self).__init__()\n",
    "        self.delta = delta\n",
    "        self.huber_loss = nn.HuberLoss(delta=delta)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # Definir os limites para considerar positivo ou negativo\n",
    "        negative_threshold = -1.72\n",
    "        positive_threshold = 1.72\n",
    "\n",
    "        # Verificar os sinais de output e target usando os limites fornecidos\n",
    "        output_sign = (output < negative_threshold).float() - (output > positive_threshold).float()\n",
    "        target_sign = (target < negative_threshold).float() - (target > positive_threshold).float()\n",
    "\n",
    "        # Calcular a perda Huber normal\n",
    "        loss = self.huber_loss(output, target)\n",
    "\n",
    "        # Verificar se o sinal de output é diferente de target\n",
    "        sign_diff = (output_sign != target_sign).float()\n",
    "\n",
    "        # Se os sinais forem diferentes, multiplicar a perda por 2\n",
    "        adjusted_loss = loss * (1 + sign_diff)  # Duas vezes a perda se os sinais forem diferentes\n",
    "\n",
    "        return adjusted_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2.0 3.0 (ambos positivos): 0.5\n",
      "Loss -2.0 -3.0 (ambos negativos): 0.5\n",
      "Loss 2.0 -3.0 (output positivo, target negativo): 9.0\n",
      "Loss -2.0 3.0 (output negativo, target positivo): 9.0\n",
      "Loss 0.5 2.0 (output dentro do intervalo, target positivo): 2.0\n",
      "Loss -0.5 -2.0 (output dentro do intervalo, target negativo): 2.0\n",
      "Loss -0.5 0.5 (sinais diferentes dentro do intervalo): 0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Função de teste da CustomHuberLoss\n",
    "def test_all_sign_cases():\n",
    "    # Instanciar a perda customizada\n",
    "    criterion = CustomHuberLoss(delta=1.0)\n",
    "\n",
    "    # Exemplo 1: Ambos positivos\n",
    "    output1 = torch.tensor([2.0])  # Positivo\n",
    "    target1 = torch.tensor([3.0])  # Positivo\n",
    "    loss1 = criterion(output1, target1)\n",
    "    print(f\"Loss 2.0 3.0 (ambos positivos): {loss1.item()}\")  # Esperado: perda normal\n",
    "\n",
    "    # Exemplo 2: Ambos negativos\n",
    "    output2 = torch.tensor([-2.0])  # Negativo\n",
    "    target2 = torch.tensor([-3.0])  # Negativo\n",
    "    loss2 = criterion(output2, target2)\n",
    "    print(f\"Loss -2.0 -3.0 (ambos negativos): {loss2.item()}\")  # Esperado: perda normal\n",
    "\n",
    "    # Exemplo 3: Output positivo, target negativo\n",
    "    output3 = torch.tensor([2.0])  # Positivo\n",
    "    target3 = torch.tensor([-3.0])  # Negativo\n",
    "    loss3 = criterion(output3, target3)\n",
    "    print(f\"Loss 2.0 -3.0 (output positivo, target negativo): {loss3.item()}\")  # Esperado: perda * 2\n",
    "\n",
    "    # Exemplo 4: Output negativo, target positivo\n",
    "    output4 = torch.tensor([-2.0])  # Negativo\n",
    "    target4 = torch.tensor([3.0])  # Positivo\n",
    "    loss4 = criterion(output4, target4)\n",
    "    print(f\"Loss -2.0 3.0 (output negativo, target positivo): {loss4.item()}\")  # Esperado: perda * 2\n",
    "\n",
    "    # Exemplo 5: Output dentro do intervalo, target positivo\n",
    "    output5 = torch.tensor([0.5])  # Dentro do intervalo\n",
    "    target5 = torch.tensor([2.0])  # Positivo\n",
    "    loss5 = criterion(output5, target5)\n",
    "    print(f\"Loss 0.5 2.0 (output dentro do intervalo, target positivo): {loss5.item()}\")  # Esperado: perda normal\n",
    "\n",
    "    # Exemplo 6: Output dentro do intervalo, target negativo\n",
    "    output6 = torch.tensor([-0.5])  # Dentro do intervalo\n",
    "    target6 = torch.tensor([-2.0])  # Negativo\n",
    "    loss6 = criterion(output6, target6)\n",
    "    print(f\"Loss -0.5 -2.0 (output dentro do intervalo, target negativo): {loss6.item()}\")  # Esperado: perda normal\n",
    "\n",
    "    # Exemplo 7: Ambos dentro do intervalo, mas sinais diferentes\n",
    "    output7 = torch.tensor([-0.5])  # Dentro do intervalo, negativo\n",
    "    target7 = torch.tensor([0.5])  # Dentro do intervalo, positivo\n",
    "    loss7 = criterion(output7, target7)\n",
    "    print(f\"Loss -0.5 0.5 (sinais diferentes dentro do intervalo): {loss7.item()}\")  # Esperado: perda * 2\n",
    "\n",
    "test_all_sign_cases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import torch\n",
    "\n",
    "# Carregar o conjunto de dados de avaliação (STS Benchmark)\n",
    "eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
    "\n",
    "# Listas para armazenar as pontuações reais e as previsões do modelo\n",
    "true_scores = []\n",
    "pred_scores = []\n",
    "\n",
    "# Iterar sobre os exemplos no conjunto de dados\n",
    "for example in eval_dataset:\n",
    "    # Tokenização combinada\n",
    "    encoding = embedding_generator.tokenizer(\n",
    "        [example['sentence1'], example['sentence2']],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,  # Padding dinâmico\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Converter para o dispositivo\n",
    "    input_ids1 = encoding['input_ids'][0]\n",
    "    attention_mask1 = encoding['attention_mask'][0]\n",
    "    input_ids2 = encoding['input_ids'][1]\n",
    "    attention_mask2 = encoding['attention_mask'][1]\n",
    "\n",
    "    # Obter embeddings\n",
    "    embeddings1 = embedding_generator.weights[input_ids1]\n",
    "    embeddings2 = embedding_generator.weights[input_ids2]        \n",
    "    \n",
    "    # Concatenar as duas sequências no eixo 1\n",
    "    embeddings = torch.stack([embeddings1, embeddings2]).float().unsqueeze(0)  # (batch_size, 2, seq_length, vector_size)\n",
    "    \n",
    "    # Calcular comprimentos\n",
    "    lengths1 = attention_mask1.sum(dim=0)\n",
    "    lengths2 = attention_mask2.sum(dim=0)\n",
    "    lengths = torch.stack([lengths1, lengths2]).float().unsqueeze(0)  # (batch_size, 2)\n",
    "\n",
    "    # Converter targets\n",
    "    score = torch.tensor(example['score'], dtype=torch.float16).unsqueeze(0)  # (batch_size,)\n",
    "\n",
    "    # Obter a previsão do modelo\n",
    "    prediction = model(embeddings, lengths)\n",
    "    pred_score = 1 / (1 + abs(prediction))\n",
    "\n",
    "    # Armazenar as pontuações\n",
    "    true_scores.append(score.item())\n",
    "    pred_scores.append(pred_score.item())\n",
    "\n",
    "# Calcular as métricas de correlação\n",
    "pearson_corr, _ = pearsonr(true_scores, pred_scores)\n",
    "spearman_corr, _ = spearmanr(true_scores, pred_scores)\n",
    "\n",
    "print(\"Similaridade Inversa da Diferença Absoluta\")\n",
    "print(f\"Correlação de Pearson: {pearson_corr:.4f}\")\n",
    "print(f\"Correlação de Spearman: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tqdm import tqdm  # Para acompanhar o progresso\n",
    "\n",
    "# Carregar o conjunto de dados de avaliação (STS Benchmark)\n",
    "eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
    "\n",
    "# Definir o intervalo de valores de k a serem testados\n",
    "k_values = np.arange(0.001, 100.0, 0.01)\n",
    "best_k = None\n",
    "best_pearson = -1  # Inicialização com um valor baixo\n",
    "best_spearman = -1\n",
    "\n",
    "# Pré-computar todas as predições para evitar recalcular múltiplas vezes\n",
    "true_scores = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor example in tqdm(eval_dataset, desc=\"Processando exemplos\"):\n",
    "\t\t# Tokenização combinada\n",
    "\t\tencoding = embedding_generator.tokenizer(\n",
    "\t\t\t[example['sentence1'], example['sentence2']],\n",
    "\t\t\treturn_tensors=\"pt\",\n",
    "\t\t\tpadding=True,  # Padding dinâmico\n",
    "\t\t\ttruncation=True\n",
    "\t\t)\n",
    "\n",
    "\t\t# Converter para o dispositivo\n",
    "\t\tinput_ids1 = encoding['input_ids'][0]\n",
    "\t\tattention_mask1 = encoding['attention_mask'][0]\n",
    "\t\tinput_ids2 = encoding['input_ids'][1]\n",
    "\t\tattention_mask2 = encoding['attention_mask'][1]\n",
    "\n",
    "\t\t# Obter embeddings\n",
    "\t\tembeddings1 = embedding_generator.weights[input_ids1]\n",
    "\t\tembeddings2 = embedding_generator.weights[input_ids2]        \n",
    "\t\t\n",
    "\t\t# Concatenar as duas sequências no eixo 1\n",
    "\t\tembeddings = torch.stack([embeddings1, embeddings2]).float().unsqueeze(0)  # (batch_size, 2, seq_length, vector_size)\n",
    "\t\t\n",
    "\t\t# Calcular comprimentos\n",
    "\t\tlengths1 = attention_mask1.sum(dim=0)\n",
    "\t\tlengths2 = attention_mask2.sum(dim=0)\n",
    "\t\tlengths = torch.stack([lengths1, lengths2]).float().unsqueeze(0)  # (batch_size, 2)\n",
    "\n",
    "\t\t# Converter targets\n",
    "\t\tscore = torch.tensor(example['score'], dtype=torch.float16).unsqueeze(0)  # (batch_size,)\n",
    "\n",
    "\t\t# Obter a previsão do modelo\n",
    "\t\tprediction = model(embeddings, lengths)\n",
    "\t\tpredictions.append(prediction.item())\n",
    "\n",
    "\t\t# Armazenar as pontuações reais\n",
    "\t\ttrue_scores.append(score)\n",
    "\n",
    "true_scores = np.array(true_scores)\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Iterar sobre os valores de k para encontrar o melhor\n",
    "for k in tqdm(k_values, desc=\"Buscando o melhor k\"):\n",
    "\tpred_scores = np.exp(-k * np.abs(predictions))\n",
    "\t\n",
    "\tpearson_corr, _ = pearsonr(true_scores, pred_scores)\n",
    "\tspearman_corr, _ = spearmanr(true_scores, pred_scores)\n",
    "\t\n",
    "\t# Verificar se este k é o melhor até agora\n",
    "\tif pearson_corr > best_pearson:\n",
    "\t\tbest_pearson = pearson_corr\n",
    "\t\tbest_spearman = spearman_corr\n",
    "\t\tbest_k = k\n",
    "\n",
    "print(\"Similaridade Baseada em Exponencial\")\n",
    "print(f\"Melhor valor de k: {best_k}\")\n",
    "print(f\"Correlação de Pearson: {best_pearson:.4f}\")\n",
    "print(f\"Correlação de Spearman: {best_spearman:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import load_dataset\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Verificar se CUDA está disponível\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Carregar o modelo SentenceTransformer\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\").to(device)\n",
    "\n",
    "# Carregar o conjunto de dados de avaliação (STS Benchmark)\n",
    "eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
    "\n",
    "# Listas para armazenar as pontuações reais e as previsões do modelo\n",
    "true_scores = []\n",
    "pred_scores = []\n",
    "\n",
    "# Obter as sentenças e os escores do conjunto de dados\n",
    "sentences1 = eval_dataset['sentence1']\n",
    "sentences2 = eval_dataset['sentence2']\n",
    "scores = eval_dataset['score']\n",
    "\n",
    "# Processar uma frase por vez\n",
    "for i in range(len(eval_dataset)):\n",
    "    sentence1 = sentences1[i]\n",
    "    sentence2 = sentences2[i]\n",
    "    true_score = scores[i]\n",
    "\n",
    "    # Gerar embeddings para ambas as sentenças individualmente\n",
    "    embedding1 = model.encode(sentence1, convert_to_tensor=True, device=device, show_progress_bar=False)\n",
    "    embedding2 = model.encode(sentence2, convert_to_tensor=True, device=device, show_progress_bar=False)\n",
    "\n",
    "    # Calcular a similaridade cosseno\n",
    "    cosine_score = util.cos_sim(embedding1, embedding2).item()\n",
    "\n",
    "    # Escalar a similaridade cosseno de [-1, 1] para [0, 1]\n",
    "    scaled_score = (cosine_score + 1) / 2\n",
    "\n",
    "    # Armazenar as pontuações\n",
    "    true_scores.append(true_score)\n",
    "    pred_scores.append(scaled_score)\n",
    "\n",
    "# Calcular as métricas de correlação\n",
    "pearson_corr, _ = pearsonr(true_scores, pred_scores)\n",
    "spearman_corr, _ = spearmanr(true_scores, pred_scores)\n",
    "\n",
    "print(f\"Correlação de Pearson: {pearson_corr:.4f}\")\n",
    "print(f\"Correlação de Spearman: {spearman_corr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
